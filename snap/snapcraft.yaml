name: spark-client
base: core22
version: '3.4.1'
summary: Client side scripts to submit Spark jobs to a cluster.
description: |
  The spark-client snap includes the scripts spark-submit, spark-shell, pyspark and other tools for managing Apache Spark jobs.

grade: stable
confinement: strict

plugs:
  dot-kube-config:
    interface: personal-files
    read:
    - $HOME/.kube/config

environment:
  JAVA_HOME: $SNAP/usr/lib/jvm/java-11-openjdk-amd64
  PATH: $JAVA_HOME/bin:$PATH:$SNAP:$SNAP/opt/$CRAFT_PROJECT_NAME:$SNAP/opt/k8s:$SNAP/opt/spark
  KUBECONFIG: $SNAP_REAL_HOME/.kube/config
  SPARK_HOME: $SNAP/opt/spark
  SPARK_CONFS: $SNAP_DATA/etc/spark8t/
  SPARK_USER_DATA: $HOME

apps:
  service-account-registry:
    command: lib/python3.10/site-packages/spark8t/cli/service_account_registry.py
    environment:
      PYTHONPATH: $SNAP/python:$SNAP/lib/python3.10/site-packages:$SNAP/local/lib/dist-packages/:$SNAP/local/lib/dist-packages/spark8t/cli:$PYTHONPATH
    plugs:
        - network
        - home
        - dot-kube-config
  spark-submit:
    command: lib/python3.10/site-packages/spark8t/cli/spark_submit.py
    environment:
      PYTHONPATH: $SNAP/python:$SNAP/lib/python3.10/site-packages:$PYTHONPATH
    plugs:
        - network
        - home
        - dot-kube-config
  spark-shell:
    command: lib/python3.10/site-packages/spark8t/cli/spark_shell.py
    environment:
      PYTHONPATH: $SNAP/python:$SNAP/lib/python3.10/site-packages:$PYTHONPATH
    plugs:
        - network
        - network-bind
        - home
        - dot-kube-config
  pyspark:
    command: lib/python3.10/site-packages/spark8t/cli/pyspark.py
    environment:
      PYTHONPATH: $SNAP/python:$SNAP/lib/python3.10/site-packages:$PYTHONPATH
    plugs:
        - network
        - network-bind
        - home
        - dot-kube-config

parts:

  spark8t-conf:
    plugin: dump
    source: snap/local

  spark8t:
    plugin: python
    python-packages:
        - https://github.com/canonical/spark-k8s-toolkit-py/releases/download/v0.0.1/spark8t-0.0.1-py3-none-any.whl
    source: .
    build-packages:
        - python3
        - pip
    override-build: |
        craftctl default
        # Scripts must be executable
        chmod -R 755 $CRAFT_PART_INSTALL/lib/python3.10/site-packages/spark8t/cli/

  spark:
    plugin: nil
    build-packages:
        - ca-certificates
        - ca-certificates-java
        - openjdk-11-jre-headless
        - wget
    stage-packages:
        - openjdk-11-jre-headless
    override-build: |
        SPARK_HADOOP_VERSION='3'
        AWS_JAVA_SDK_BUNDLE_VERSION='1.11.874'
        HADOOP_AWS_VERSION='3.2.2'
        SPARK_VERSION=$(cat $CRAFT_PROJECT_DIR/SPARK_VERSION | tr -d '\n')
        TAG=$( echo ${SPARK_VERSION%-*} )
        TARBALL_URL="https://github.com/canonical/central-uploader/releases/download/spark-${TAG}/spark-${SPARK_VERSION}-bin-k8s.tgz"
        CHECKSUM_URL="${TARBALL_URL}.sha512"

        STATUSCODE=$(curl --silent --head $TARBALL_URL | head -n 1 | cut -d' ' -f2)
        
        if  [[ ${STATUSCODE} -gt 400 ]]
          then
            echo "ERROR: Latest available Spark version spark-${SPARK_VERSION} does not have a downloadable binary! Exiting...."
            exit 1
        fi
        echo "Downloading latest available Spark version spark-${SPARK_VERSION}."
        wget $TARBALL_URL
        wget $CHECKSUM_URL
        sha512sum --check "spark-${SPARK_VERSION}-bin-k8s.tgz.sha512"
        if  [[ $? -ne 0 ]]
          then
            echo "DOWNLOAD ERROR: Latest available Spark version spark-${SPARK_VERSION} could not be downloaded properly! Exiting...."
            exit 1
        fi
        tar -zxf "spark-${SPARK_VERSION}-bin-k8s.tgz.tgz"
        cd "spark-${SPARK_VERSION}-bin-k8s/jars"
        wget "https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/${AWS_JAVA_SDK_BUNDLE_VERSION}/aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar"
        wget "https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/${AWS_JAVA_SDK_BUNDLE_VERSION}/aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar.sha1"        
        echo "`cat aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar.sha1`  aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar" | sha1sum --check
        if  [[ $? -ne 0 ]]
          then
            echo "DOWNLOAD ERROR: aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar could not be downloaded properly! Exiting...."
            exit 1
        fi
        wget "https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/${HADOOP_AWS_VERSION}/hadoop-aws-${HADOOP_AWS_VERSION}.jar"
        wget "https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/${HADOOP_AWS_VERSION}/hadoop-aws-${HADOOP_AWS_VERSION}.jar.sha1"
        echo "`cat hadoop-aws-${HADOOP_AWS_VERSION}.jar.sha1`  hadoop-aws-${HADOOP_AWS_VERSION}.jar" | sha1sum --check
        if  [[ $? -ne 0 ]]
          then
            echo "DOWNLOAD ERROR: hadoop-aws-${HADOOP_AWS_VERSION}.jar could not be downloaded properly! Exiting...."
            exit 1
        fi
        cd ..
        SPARK_DIR=$CRAFT_PART_INSTALL/opt/spark
        mkdir -p $SPARK_DIR/bin
        cp -r bin/* $SPARK_DIR/bin
        mkdir -p $SPARK_DIR/jars
        cp -r jars/* $SPARK_DIR/jars
        mkdir -p $SPARK_DIR/python
        cp -r python/* $SPARK_DIR/python/

    override-prime: |
        snapcraftctl prime
        rm -vf usr/lib/jvm/java-11-openjdk-*/lib/security/blacklisted.certs

  kubectl:
    plugin: nil
    build-packages:
        - curl
    source: .
    source-type: local
    override-build: |
      craftctl default
      curl -LO -s "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
      chmod +x kubectl
      K8S_DIR=$CRAFT_PART_INSTALL/opt/k8s
      mkdir -p $K8S_DIR
      cp kubectl $K8S_DIR
